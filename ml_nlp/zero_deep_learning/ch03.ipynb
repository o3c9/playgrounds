{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "MathJax.Hub.Config({\n    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n});\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "MathJax.Hub.Queue(\n  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n  [\"PreProcess\", MathJax.Hub],\n  [\"Reprocess\", MathJax.Hub]\n);\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "\n",
    "パーセプトロンは，複雑な振る舞いをする関数であっても重みやバイアスを変えるだけで表現できる可能性を秘めていたが，問題としては，その重みを人間が与えていたことが課題であった．\n",
    "\n",
    "適切な重みをデータから自動で学習できると言うのがニューラルネットワークの重要な性質の1つである。\n",
    "ニューロン同士のつながり方に関して言えば，パーセプトロンと何ら変わることはない．\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = \\begin{cases}\n",
    "    0 \\quad(b + w_1 x_1 + w_2 x_2 \\leqq 0) \\\\\n",
    "    1 \\quad(b + w_1 x_1 + w_2 x_2 > 0) \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "を\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = h(b + w_1 x_1 + w_2 x_2)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "とおくと，\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h(x) = \\begin{cases}\n",
    "    0 \\quad(x \\leqq 0) \\\\\n",
    "    1 \\quad(x > 0) \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "とかきなおせる．この$h(x)$を活性化関数と呼ぶ.\n",
    "\n",
    "パーセプトロンでは，この活性化関数に，閾値を超えれば，On/Offを切り替える**ステップ関数**を使っているといえる．\n",
    "\n",
    "また，ニューラルネットワークでよく使われる活性化関数として，\n",
    "\n",
    "### シグモイド関数\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h(x) = \\frac{1}{1 + exp(-x)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h(x) = \\begin{cases}\n",
    "x\\quad(x>0) \\\\\n",
    "0\\quad(x\\leqq0) \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "重要なこととして，ニューラルネットワークでは，活性化関数に非線形関数を用いる必要があることが挙げられる．線形関数を用いると，隠れ層を用いなくても同じことを行うネットワークが必ず存在するため，結局隠れ層を使う意味がなくなってしまうからである．\n",
    "\n",
    "これは，$h(x) = cx$とすると，たとえば３層の場合, $y = h(h(h(x))$となるが，展開して，$y=c^3x$と書けてしまうから，というのがわかりやすい説明だろう．\n",
    "\n",
    "以下，活性化関数の実装をしていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 1]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpyを使うことで，numpyの配列（ベクトル）や行列も受け取ることができるようになる\n",
    "def step_function(x):\n",
    "    y = x > 0\n",
    "    return y.astype(np.int)\n",
    "\n",
    "print(step_function(np.array([-1.0, 0.5, 1.0])))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}