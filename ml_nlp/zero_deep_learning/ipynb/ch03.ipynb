{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "if(MathJax) {\n    MathJax.Hub.Config({\n        TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n    });\n}\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%javascript\n",
    "if(MathJax) {\n",
    "    MathJax.Hub.Config({\n",
    "        TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "    });\n",
    "}\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "if(MathJax) {\n  MathJax.Hub.Queue(\n    [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n    [\"PreProcess\", MathJax.Hub],\n    [\"Reprocess\", MathJax.Hub]\n  );\n}\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%javascript\n",
    "if(MathJax) {\n",
    "  MathJax.Hub.Queue(\n",
    "    [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "    [\"PreProcess\", MathJax.Hub],\n",
    "    [\"Reprocess\", MathJax.Hub]\n",
    "  );\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "\n",
    "パーセプトロンは，複雑な振る舞いをする関数であっても重みやバイアスを変えるだけで表現できる可能性を秘めていたが，問題としては，その重みを人間が与えていたことが課題であった．\n",
    "\n",
    "適切な重みをデータから自動で学習できると言うのがニューラルネットワークの重要な性質の1つである。\n",
    "ニューロン同士のつながり方に関して言えば，パーセプトロンと何ら変わることはない．\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = \\begin{cases}\n",
    "    0 \\quad(b + w_1 x_1 + w_2 x_2 \\leqq 0) \\\\\n",
    "    1 \\quad(b + w_1 x_1 + w_2 x_2 > 0) \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "を\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = h(b + w_1 x_1 + w_2 x_2)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "とおくと，\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h(x) = \\begin{cases}\n",
    "    0 \\quad(x \\leqq 0) \\\\\n",
    "    1 \\quad(x > 0) \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "とかきなおせる．この$h(x)$を活性化関数と呼ぶ.\n",
    "\n",
    "パーセプトロンでは，この活性化関数に，閾値を超えれば，On/Offを切り替える**ステップ関数**を使っているといえる．\n",
    "\n",
    "また，ニューラルネットワークでよく使われる活性化関数として，\n",
    "\n",
    "### シグモイド関数\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h(x) = \\frac{1}{1 + exp(-x)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h(x) = \\begin{cases}\n",
    "x\\quad(x>0) \\\\\n",
    "0\\quad(x\\leqq0) \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "重要なこととして，ニューラルネットワークでは，活性化関数に非線形関数を用いる必要があることが挙げられる．線形関数を用いると，隠れ層を用いなくても同じことを行うネットワークが必ず存在するため，結局隠れ層を使う意味がなくなってしまうからである．\n",
    "\n",
    "これは，$h(x) = cx$とすると，たとえば３層の場合, $y = h(h(h(x))$となるが，展開して，$y=c^3x$と書けてしまうから，というのがわかりやすい説明だろう．\n",
    "\n",
    "以下，活性化関数の実装をしていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 1]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpyを使うことで，numpyの配列（ベクトル）や行列も受け取ることができるようになる\n",
    "def step_function(x):\n",
    "    y = x > 0\n",
    "    return y.astype(np.int)\n",
    "\n",
    "print(step_function(np.array([-1.0, 0.5, 1.0])))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの実装\n",
    "\n",
    "ここでは２層の隠れ層を持つ，３層ニューラルネットワークを構築する\n",
    "\n",
    "記号の確認\n",
    "\n",
    "1. $w^{(1)}_{12}$は，入力の２番目($x_2$)から，１層目の第一ニューロンへの重みを表す\n",
    "2. $b^{(1)}_1$は，１層目の第一ニューロンへのバイアスを表す\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a^{(1)}_1 = w^{(1)}_{11} x_1 + w^{(1)}_{12} x_2 + b^{(1)}_1\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "z^{(1)}_1 = h(a^{(1)}_1)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a^{(2)}_1 = w^{(2)}_{11} a^{(1)}_1 + w^{(2)}_{12} a^{(1)}_2 + w^{(2)}_{13} a^{(1)}_3 + b^{(2)}_1\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "z^{(2)}_1 = h(a^{(2)}_1)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a^{(3)}_1 = w^{(3)}_{11} a^{(2)}_1 + w^{(3)}_{12} a^{(2)}_2 + b^{(3)}_1\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_1 = \\sigma(a^{(3)}_1)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "出力層の活性化関数である，$\\sigma(x)$は, $h(x)$とは別物である．\n",
    "以下の例では，`identity_function`が，それにあたる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.31682708 0.69627909]\n"
    }
   ],
   "source": [
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['b1'] = np.array([0.1,0.2,0.3])\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b2'] = np.array([0.1,0.2])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b3'] = np.array([0.1,0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identity_function(a3)\n",
    "    return y\n",
    "\n",
    "n = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "print(forward(n, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力層の設計\n",
    "\n",
    "機械学習の問題は，大きく分類問題と回帰問題に分かれる．分類の場合には`softmax`, 回帰の場合には`identity`（＝恒等関数）を使われる事が多い\n",
    "\n",
    "### ソフトマックス\n",
    "\n",
    "出力を0-1の確率に変える（合計は１となる）\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_k = \\frac{exp(a_k)}{\\sum^n_{i=1} exp(a_i)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "しかし，このソフトマックスをコンピュータ上でそのまま計算すると，桁が大きくなりすぎてオーバーフローが起きてしまう．そこで，\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_k = \\frac{exp(a_k)}{\\sum^n_{i=1} exp(a_i)} = \\frac{C exp(a_k)}{C \\sum^n_{i=1} exp(a_i)} \\\\\n",
    "= \\frac{exp(a_k) + logC}{\\sum^n_{i=1} exp(a_i) + logC} \\\\\n",
    "= \\frac{exp(a_k) + C'}{\\sum^n_{i=1} exp(a_i) + C'}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "つまり，分母と分子には任意の定数を足しても結果が変わらないことを利用して，たとえば，$exp(a_i)$の最大値をあらかじめ引いてやるなどの工夫をしておく必要がある．\n",
    "\n",
    "ちなみに，ソフトマックスはそれなりに重い処理であるため，クラス分類で出力の一番大きなものだけを認識結果として使う「推論」フェーズの場合，ソフトマックスを省略して，単に，`np.max(a)`を結果とすることが多い．ソフトマックスが利用されるのは，主に学習フェーズである．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.28943311 0.31987306 0.39069383]\n"
    }
   ],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "print(softmax(np.array([0.1,0.2,0.4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}